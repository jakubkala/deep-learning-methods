{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from neural_network_wrapper import NeuralNetworkWrapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optimizers\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_simple_train_100 = pd.read_csv(\"./projekt1/classification/data.simple.train.100.csv\")\n",
    "data_simple_train_500 = pd.read_csv(\"./projekt1/classification/data.simple.train.500.csv\")\n",
    "data_simple_train_1000 = pd.read_csv(\"./projekt1/classification/data.simple.train.1000.csv\")\n",
    "data_simple_train_10000 = pd.read_csv(\"./projekt1/classification/data.simple.train.10000.csv\")\n",
    "\n",
    "data_simple_test_100 = pd.read_csv(\"./projekt1/classification/data.simple.test.100.csv\")\n",
    "data_simple_test_500 = pd.read_csv(\"./projekt1/classification/data.simple.test.500.csv\")\n",
    "data_simple_test_1000 = pd.read_csv(\"./projekt1/classification/data.simple.test.1000.csv\")\n",
    "data_simple_test_10000 = pd.read_csv(\"./projekt1/classification/data.simple.test.10000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_simple(train_data, test_data):\n",
    "    \n",
    "    X_train = np.array(train_data.loc[:, ['x', 'y']])\n",
    "    y_train = train_data.cls\n",
    "    y_train -= 1\n",
    "    #one hot encoding\n",
    "    y_ohc = np.zeros((y_train.size, int(np.max(y_train))+1))\n",
    "    y_ohc[np.arange(y_train.size),y_train.astype(np.int)] = 1\n",
    "    y_train = y_ohc\n",
    "    \n",
    "    X_test = np.array(test_data.loc[:, ['x', 'y']])\n",
    "    y_test = test_data.cls\n",
    "    y_test -= 1\n",
    "    #one hot encoding\n",
    "    y_ohc = np.zeros((y_test.size, int(np.max(y_test))+1))\n",
    "    y_ohc[np.arange(y_test.size),y_test.astype(np.int)] = 1\n",
    "    y_test = y_ohc\n",
    "    \n",
    "    \n",
    "    # Are we supposed to use StandardScaler?\n",
    "    ss = StandardScaler()\n",
    "    X_train = ss.fit_transform(X_train)\n",
    "    X_test = ss.transform(X_test)\n",
    "    \n",
    "    return {\"X_train\": X_train,\n",
    "           \"X_test\": X_test,\n",
    "           \"y_train\": y_train,\n",
    "           \"y_test\": y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"dataset name\": \"Data simple 100 obs\",\n",
    "        \"data\": prepare_data_simple(data_simple_train_100, data_simple_test_100)},\n",
    "       {\"dataset name\": \"Data simple 500 obs\",\n",
    "        \"data\": prepare_data_simple(data_simple_train_500, data_simple_test_500)},\n",
    "       {\"dataset name\": \"Data simple 1000 obs\",\n",
    "        \"data\": prepare_data_simple(data_simple_train_1000, data_simple_test_1000)},\n",
    "       {\"dataset name\": \"Data simple 10000 obs\",\n",
    "        \"data\": prepare_data_simple(data_simple_train_10000, data_simple_test_10000)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "neuron_numbers = [4, 4, 2]\n",
    "output_activation = ['sigmoid']\n",
    "seed=42 #reproducibility\n",
    "num_epochs=5\n",
    "loss_function = 'logistic_loss'\n",
    "learning_rates = {\"LR = 0.1\" : 0.1,\n",
    "                 \"LR = 0.01\" : 0.01,\n",
    "                 \"LR = 0.001\" : 0.001}\n",
    "                  \n",
    "\n",
    "\n",
    "# Things we would like to test\n",
    "activation_functions = ['relu', 'leaky_relu']\n",
    "batch_size = 1#[2*(i+1) for i in range(5)]\n",
    "optimizer_beta = [0, 0.5, 0.9] # 0 means no inertia\n",
    "\n",
    "\n",
    "experiment_dict = {\n",
    "    \"input_dim\" : input_dim,\n",
    "    \"neuron_numbers\" : neuron_numbers, # number of neurons in consecutive layers\n",
    "    \"activation_functions\" : activation_functions,\n",
    "    \"loss_function\" : loss_function,\n",
    "    \"learning_rates\" : learning_rates,\n",
    "    \"batch_size\" : batch_size,\n",
    "    \"num_epochs\" : num_epochs,\n",
    "    \"seed\" : seed,\n",
    "    \"output_activation\" : output_activation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_experiment(dataset,\n",
    "                       d,\n",
    "                       exp_objective,\n",
    "                       exp_values,\n",
    "                       num_reps):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    X_train = dataset['X_train']\n",
    "    y_train = dataset['y_train']\n",
    "    X_test = dataset['X_test']\n",
    "    y_test = dataset['y_test']\n",
    "\n",
    "    d = d.copy()\n",
    "    \n",
    "    for k in exp_values.keys():\n",
    "        d[k] = {}\n",
    "        d[k]['test_accuracy'] = []\n",
    "    \n",
    "    for i in range(num_reps):\n",
    "        print(f\"Experiment {i+1}/{num_reps}\")\n",
    "    \n",
    "        # reproducibility issues\n",
    "        random.seed(d['seed'] + i)\n",
    "        np.random.seed(d['seed'] + i)\n",
    "    \n",
    "        # testing learning rate\n",
    "        if exp_objective == 'lr':\n",
    "            for k, v in exp_values.items():\n",
    "            \n",
    "                \n",
    "\n",
    "                NN = NeuralNetworkWrapper(d['input_dim'],\n",
    "                                      d['neuron_numbers'],\n",
    "                                      ['relu'] * (len(d['neuron_numbers']) - 1) + d['output_activation'],\n",
    "                                      d['loss_function'],\n",
    "                                      v,\n",
    "                                      optimizers.Optimizer(),\n",
    "                                      d['batch_size'])\n",
    "                \n",
    "                NN.train(X_train,\n",
    "                        y_train,\n",
    "                        d['num_epochs'],\n",
    "                        validation_split = 0,\n",
    "                        test_accuracy=(X_test, y_test),\n",
    "                        verbosity=False)\n",
    "                \n",
    "                d[k]['test_accuracy'].append(NN.test_accuracy)\n",
    "\n",
    "    for k in exp_values.keys():\n",
    "        # aggregating results\n",
    "        d[k]['test_accuracy_mean'] = np.mean(np.array(d[k]['test_accuracy']).T, axis=1)\n",
    "        d[k]['test_accuracy_std'] = np.std(np.array(d[k]['test_accuracy']).T, axis=1)\n",
    "\n",
    "        d[k] = {\"Accuracy\": d[k]['test_accuracy_mean'],\n",
    "               \"Accuracy std\": d[k]['test_accuracy_std'],\n",
    "               \"Best Accuracy\": np.max(d[k]['test_accuracy_mean']),\n",
    "               \"Best Accuracy std\": d[k]['test_accuracy_std'][np.argmax(d[k]['test_accuracy_mean'])]}\n",
    "\n",
    "    return {k: d[k] for k in exp_values.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform_experiment(data[0]['data'],\n",
    "#                    experiment_dict,\n",
    "#                    'lr',\n",
    "#                    {'lr=0.1' : 0.001,\n",
    "#                    'lr=0.2' : 0.002},\n",
    "#                   10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments_pipeline(data,\n",
    "                         experiment_dict,\n",
    "                         num_reps=1,\n",
    "                         save_to_file=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    d = experiment_dict.copy()\n",
    "    output = {}\n",
    "    # Experiments for each dataset\n",
    "    for dataset in data:\n",
    "        output[dataset['dataset name']] = perform_experiment(dataset['data'],\n",
    "                                                     experiment_dict,\n",
    "                                                     'lr',\n",
    "                                                     {'lr=0.1' : 0.001,\n",
    "                                                      'lr=0.2' : 0.002},\n",
    "                                                     num_reps)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1/5\n",
      "Final loss: 1.343\n",
      "Final loss: 0.641\n",
      "Experiment 2/5\n",
      "Final loss: 0.941\n",
      "Final loss: 1.125\n",
      "Experiment 3/5\n",
      "Final loss: 1.797\n",
      "Final loss: 0.801\n",
      "Experiment 4/5\n",
      "Final loss: 1.330\n",
      "Final loss: 1.010\n",
      "Experiment 5/5\n",
      "Final loss: 1.862\n",
      "Final loss: 1.242\n",
      "Experiment 1/5\n",
      "Final loss: 0.929\n",
      "Final loss: 0.239\n",
      "Experiment 2/5\n",
      "Final loss: 0.302\n",
      "Final loss: 0.414\n",
      "Experiment 3/5\n",
      "Final loss: 1.419\n",
      "Final loss: 0.177\n",
      "Experiment 4/5\n",
      "Final loss: 0.903\n",
      "Final loss: 0.309\n",
      "Experiment 5/5\n",
      "Final loss: 0.527\n",
      "Final loss: 0.248\n",
      "Experiment 1/5\n",
      "Final loss: 0.569\n",
      "Final loss: 0.120\n",
      "Experiment 2/5\n",
      "Final loss: 0.164\n",
      "Final loss: 0.148\n",
      "Experiment 3/5\n",
      "Final loss: 1.149\n",
      "Final loss: 0.087\n",
      "Experiment 4/5\n",
      "Final loss: 0.512\n",
      "Final loss: 0.136\n",
      "Experiment 5/5\n",
      "Final loss: 0.335\n",
      "Final loss: 0.123\n",
      "Experiment 1/5\n",
      "Final loss: 0.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakubkala/IAD/semestr-2/mgu/deep-learning-methods/loss_functions.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = (-1 / m) * (np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)))\n",
      "/home/jakubkala/IAD/semestr-2/mgu/deep-learning-methods/loss_functions.py:5: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = (-1 / m) * (np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)))\n",
      "/home/jakubkala/IAD/semestr-2/mgu/deep-learning-methods/loss_functions.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.divide(1 - y, 1 - y_hat) - np.divide(y, y_hat)\n",
      "/home/jakubkala/IAD/semestr-2/mgu/deep-learning-methods/activation_functions.py:21: RuntimeWarning: invalid value encountered in greater\n",
      "  derivative[Z > 0] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: nan\n",
      "Experiment 2/5\n",
      "Final loss: 0.037\n",
      "Final loss: nan\n",
      "Experiment 3/5\n",
      "Final loss: nan\n",
      "Final loss: nan\n",
      "Experiment 4/5\n",
      "Final loss: 0.075\n",
      "Final loss: nan\n",
      "Experiment 5/5\n",
      "Final loss: nan\n",
      "Final loss: nan\n"
     ]
    }
   ],
   "source": [
    "ans = experiments_pipeline(data, experiment_dict, num_reps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Data simple 100 obs': {'lr=0.1': {'Accuracy': array([0.53 , 0.554, 0.562, 0.59 , 0.644]),\n",
       "   'Accuracy std': array([0.07823043, 0.08979978, 0.09846827, 0.1154123 , 0.15994999]),\n",
       "   'Best Accuracy': 0.6439999999999999,\n",
       "   'Best Accuracy std': 0.15994999218505765},\n",
       "  'lr=0.2': {'Accuracy': array([0.604, 0.686, 0.73 , 0.76 , 0.812]),\n",
       "   'Accuracy std': array([0.10442222, 0.11808472, 0.14042792, 0.13696715, 0.07547185]),\n",
       "   'Best Accuracy': 0.812,\n",
       "   'Best Accuracy std': 0.07547184905645281}},\n",
       " 'Data simple 500 obs': {'lr=0.1': {'Accuracy': array([0.5976, 0.7556, 0.8044, 0.8272, 0.842 ]),\n",
       "   'Accuracy std': array([0.11837838, 0.140692  , 0.15813614, 0.17005223, 0.176     ]),\n",
       "   'Best Accuracy': 0.842,\n",
       "   'Best Accuracy std': 0.176},\n",
       "  'lr=0.2': {'Accuracy': array([0.8228, 0.9   , 0.942 , 0.9712, 0.9772]),\n",
       "   'Accuracy std': array([0.06961149, 0.02356268, 0.01507315, 0.02278947, 0.01916664]),\n",
       "   'Best Accuracy': 0.9772000000000001,\n",
       "   'Best Accuracy std': 0.01916663768113752}},\n",
       " 'Data simple 1000 obs': {'lr=0.1': {'Accuracy': array([0.751 , 0.816 , 0.8486, 0.9106, 0.937 ]),\n",
       "   'Accuracy std': array([0.13567314, 0.15629587, 0.17195651, 0.09331366, 0.06653721]),\n",
       "   'Best Accuracy': 0.937,\n",
       "   'Best Accuracy std': 0.0665372076360287},\n",
       "  'lr=0.2': {'Accuracy': array([0.8928, 0.9598, 0.9764, 0.9836, 0.9872]),\n",
       "   'Accuracy std': array([0.02839296, 0.02726463, 0.01674037, 0.00960417, 0.0059127 ]),\n",
       "   'Best Accuracy': 0.9872,\n",
       "   'Best Accuracy std': 0.0059126981996377986}},\n",
       " 'Data simple 10000 obs': {'lr=0.1': {'Accuracy': array([0.97722, 0.79128, 0.79368, 0.59558, 0.5957 ]),\n",
       "   'Accuracy std': array([0.02184421, 0.39564729, 0.39684286, 0.48629177, 0.48638985]),\n",
       "   'Best Accuracy': 0.97722,\n",
       "   'Best Accuracy std': 0.021844212048046062},\n",
       "  'lr=0.2': {'Accuracy': array([0.99   , 0.19806, 0.19814, 0.19824, 0.     ]),\n",
       "   'Accuracy std': array([0.0020601, 0.39612  , 0.39628  , 0.39648  , 0.       ]),\n",
       "   'Best Accuracy': 0.99,\n",
       "   'Best Accuracy std': 0.0020600970850909034}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fig, a = plt.subplots(2, 2)\n",
    "\n",
    "# #plt.figure(figsize=(21,12))\n",
    "\n",
    "# a[0][0].plot([i+1 for i in range(d['num_epochs'])], d['train_loss_mean'], label = d['experiment_name'])\n",
    "# a[0][1].plot([i+1 for i in range(d['num_epochs'])], d['valid_loss_mean'], label = d['experiment_name'])\n",
    "# a[1][0].plot([i+1 for i in range(d['num_epochs'])], d['train_accuracy_mean'], label = d['experiment_name'])\n",
    "# a[1][1].plot([i+1 for i in range(d['num_epochs'])], d['valid_accuracy_mean'], label = d['experiment_name'])\n",
    "\n",
    "# a[0][0].plot([i+1 for i in range(d1['num_epochs'])], d1['train_loss_mean'], label = d1['experiment_name'])\n",
    "# a[0][1].plot([i+1 for i in range(d1['num_epochs'])], d1['valid_loss_mean'], label = d1['experiment_name'])\n",
    "# a[1][0].plot([i+1 for i in range(d1['num_epochs'])], d1['train_accuracy_mean'], label = d1['experiment_name'])\n",
    "# a[1][1].plot([i+1 for i in range(d1['num_epochs'])], d1['valid_accuracy_mean'], label = d1['experiment_name'])\n",
    "\n",
    "# a[0][1].legend(loc=\"upper right\")\n",
    "\n",
    "# a[0][0].title.set_text('Train Loss')\n",
    "# a[0][1].title.set_text('Validation Loss')\n",
    "# a[1][0].title.set_text('Train Accuracy')\n",
    "# a[1][1].title.set_text('Validation Accuracy')\n",
    "\n",
    "# fig.set_figheight(9)\n",
    "# fig.set_figwidth(15)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
