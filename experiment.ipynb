{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from neural_network_wrapper import NeuralNetworkWrapper\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optimizers\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./projekt1/classification/data.simple.train.1000.csv\")\n",
    "\n",
    "X = np.array(data.loc[:, ['x', 'y']])\n",
    "y = data.cls\n",
    "y -= 1\n",
    "#one hot encoding\n",
    "y_ohc = np.zeros((y.size, int(np.max(y))+1))\n",
    "y_ohc[np.arange(y.size),y.astype(np.int)] = 1\n",
    "y = y_ohc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "neuron_numbers = [4, 4, 2]\n",
    "activation_functions = ['relu', 'relu', 'sigmoid']\n",
    "loss_function = 'logistic_loss'\n",
    "learning_rate = 0.01\n",
    "optimizer = optimizers.Optimizer()\n",
    "batch_size = 128\n",
    "val_split = 0.1\n",
    "num_epochs=50\n",
    "seed=42\n",
    "dataset_name=\"test\"\n",
    "experiment_name=\"test\"\n",
    "\n",
    "experiment_dict = {\n",
    "    \"input_dim\" : input_dim,\n",
    "    \"neuron_numbers\" : neuron_numbers, # number of neurons in consecutive layers\n",
    "    \"activation_functions\" : activation_functions,\n",
    "    \"loss_function\" : loss_function,\n",
    "    \"learning_rate\" : learning_rate,\n",
    "    \"optimizer\" : optimizer,\n",
    "    \"batch_size\" : batch_size,\n",
    "    \"validation_split\" : val_split,\n",
    "    \"num_epochs\" : num_epochs,\n",
    "    \"seed\" : seed,\n",
    "    \"dataset_name\" : dataset_name,\n",
    "    \"experiment_name\" : experiment_name\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments_pipeline(X,\n",
    "                         y,\n",
    "                         experiment_dict,\n",
    "                         num_reps=1,\n",
    "                         save_to_file=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    d = experiment_dict.copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    d['loss_on_epoch'] = []\n",
    "    d['loss_on_epoch_valid'] = []\n",
    "    d['accuracy_on_epoch'] = []\n",
    "    d['accuracy_on_epoch_valid'] = []\n",
    "    \n",
    "    \n",
    "    for i in range(num_reps):\n",
    "        print(f\"Experiment {i+1}/{num_reps}\")\n",
    "    \n",
    "        # reproducibility issues\n",
    "        random.seed(d['seed'] + i)\n",
    "        np.random.seed(d['seed'] + i)\n",
    "    \n",
    "        NN = NeuralNetworkWrapper(d['input_dim'],\n",
    "                              d['neuron_numbers'],\n",
    "                              d['activation_functions'],\n",
    "                              d['loss_function'],\n",
    "                              d['learning_rate'],\n",
    "                              d['optimizer'],\n",
    "                              d['batch_size'])\n",
    "        NN.train(X,\n",
    "                  y,\n",
    "                  d['num_epochs'],\n",
    "                  d['validation_split'],\n",
    "                cache_accuracy=True,\n",
    "                verbosity=False)\n",
    "\n",
    "        d['loss_on_epoch'].append(NN.loss_on_epoch)\n",
    "        d['loss_on_epoch_valid'].append(NN.loss_on_epoch_valid)\n",
    "        d['accuracy_on_epoch'].append(NN.accuracy)\n",
    "        d['accuracy_on_epoch_valid'].append(NN.accuracy_valid)\n",
    "\n",
    "    # aggregating results\n",
    "    d['train_loss_mean'] = np.mean(np.array(output['loss_on_epoch']).T, axis=1)\n",
    "    d['valid_loss_mean'] = np.mean(np.array(output['loss_on_epoch_valid']).T, axis=1)\n",
    "    d['train_accuracy_mean'] = np.mean(np.array(output['accuracy_on_epoch']).T, axis=1)\n",
    "    d['valid_accuracy_mean'] = np.mean(np.array(output['accuracy_on_epoch_valid']).T, axis=1)\n",
    "    \n",
    "    d['train_loss_std'] = np.std(np.array(output['loss_on_epoch']).T, axis=1)\n",
    "    d['valid_loss_std'] = np.std(np.array(output['loss_on_epoch_valid']).T, axis=1)\n",
    "    d['train_accuracy_std'] = np.std(np.array(output['accuracy_on_epoch']).T, axis=1)\n",
    "    d['valid_accuracy_std'] = np.std(np.array(output['accuracy_on_epoch_valid']).T, axis=1)\n",
    "    \n",
    "    \n",
    "    #TODO: how to evaluate optimizers? Object cannot be saved to a JSON file\n",
    "    try:\n",
    "        del d['optimizer']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if save_to_file:\n",
    "        filename = d['experiment_name'] + '_' + d['dataset_name'] + '.json'\n",
    "        \n",
    "        if filename in os.listdir():\n",
    "            raise Exception(f\"File {filename} already exists!\")\n",
    "        else:\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(d, file)\n",
    "            print(\"File successfully saved!\")\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1/2\n",
      "Final loss: 0.781\n",
      "Experiment 2/2\n",
      "Final loss: 0.294\n"
     ]
    }
   ],
   "source": [
    "output = experiments_pipeline(X, y, experiment_dict, num_reps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07444444, 0.08888889, 0.09055556, 0.09777778, 0.10388889,\n",
       "       0.10777778, 0.11277778, 0.12      , 0.12222222, 0.12944444,\n",
       "       0.13277778, 0.13722222, 0.13888889, 0.14333333, 0.145     ,\n",
       "       0.14      , 0.12444444, 0.10944444, 0.09833333, 0.08888889,\n",
       "       0.07722222, 0.07277778, 0.06444444, 0.05666667, 0.05      ,\n",
       "       0.04666667, 0.04722222, 0.04277778, 0.04555556, 0.04222222,\n",
       "       0.04222222, 0.04111111, 0.04333333, 0.04277778, 0.04166667,\n",
       "       0.04333333, 0.04388889, 0.04277778, 0.04      , 0.03555556,\n",
       "       0.035     , 0.03444444, 0.03333333, 0.03222222, 0.03333333,\n",
       "       0.03111111, 0.03055556, 0.02944444, 0.02444444, 0.02444444])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['train_accuracy_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
